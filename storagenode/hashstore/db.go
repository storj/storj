// Copyright (C) 2024 Storj Labs, Inc.
// See LICENSE for copying information.

package hashstore

import (
	"context"
	"io/fs"
	"path/filepath"
	"sync"
	"time"

	"github.com/zeebo/errs"
	"github.com/zeebo/mwc"
	"go.uber.org/zap"

	"storj.io/common/memory"
	"storj.io/drpc/drpcsignal"
)

var (
	// Error is the class that wraps all errors generated by the hashstore package.
	Error = errs.Class("hashstore")

	// ErrCollision represents collision errors returned while committing to the store.
	ErrCollision = Error.New("collision detected")
)

const (
	db_MaxLoad     = 0.95 // maximum load factor of store before blocking new writes
	db_CompactLoad = 0.75 // load factor before starting compaction
)

type compactState struct {
	store  *Store
	cancel func()
	done   drpcsignal.Signal // set when compaction is done
}

// DB is a database that stores pieces.
type DB struct {
	logsPath    string // directory for log files (binary).
	tablePath   string // directory for metadata (table).
	log         *zap.Logger
	shouldTrash func(context.Context, Key, time.Time) bool
	lastRestore func(context.Context) time.Time

	closed drpcsignal.Signal // closed state
	cloErr error             // close error
	cloMu  sync.Mutex        // synchronizes closing

	wg sync.WaitGroup // waitgroup for background goroutines

	mu      sync.Mutex    // protects the following fields
	compact *compactState // set if compaction is in progress
	active  *Store        // store that currently absorbs writes
	passive *Store        // store that was being compacted
}

// New makes or opens an existing database in the directory allowing for nlogs concurrent writes.
func New(
	ctx context.Context,
	cfg Config,
	logsPath string, tablePath string, log *zap.Logger,
	shouldTrash func(context.Context, Key, time.Time) bool,
	lastRestore func(context.Context) time.Time,
) (_ *DB, err error) {
	defer mon.Task()(&ctx)(&err)

	// set default values for the optional parameters.
	if log == nil {
		log = zap.NewNop()
	}
	if lastRestore == nil {
		lastRestore = func(ctx context.Context) time.Time { return time.Time{} }
	}

	if tablePath == "" {
		tablePath = logsPath
	}
	// partially initialize the database so that we can close it if there's an error.
	d := &DB{
		logsPath:    logsPath,
		tablePath:   tablePath,
		log:         log,
		shouldTrash: shouldTrash,
		lastRestore: lastRestore,
	}
	defer func() {
		if err != nil {
			_ = d.Close()
		}
	}()

	// open the active and passive stores.
	d.active, err = NewStore(ctx, cfg, filepath.Join(logsPath, "s0"), filepath.Join(tablePath, "s0", "meta"), log.With(zap.String("store", "s0")))
	if err != nil {
		return nil, Error.Wrap(err)
	}
	d.passive, err = NewStore(ctx, cfg, filepath.Join(logsPath, "s1"), filepath.Join(tablePath, "s1", "meta"), log.With(zap.String("store", "s1")))
	if err != nil {
		return nil, Error.Wrap(err)
	}

	// make the store with the larger load active. this is so that we have more time in the other
	// store before it needs compacting when the active store eventually starts compacting. it uses
	// <= instead of < only because it slightly increases code coverage (we do the swap for empty
	// databases) at ~zero cost.
	if d.active.Load() <= d.passive.Load() {
		d.swapStoresLocked()
	}

	// if the passive store's load is too high, immediately begin compacting it. this will allow us
	// to absorb writes more quickly if the active store becomes loaded.
	if d.passive.Load() >= db_CompactLoad {
		d.beginPassiveCompaction()
	}

	// start a background goroutine to ensure that the database compacts the store at least once
	// a day to have a mechanism to clean up ttl data even if no writes are occurring.
	d.wg.Add(1)
	go d.backgroundCompactions()

	return d, nil
}

func (d *DB) swapStoresLocked() { d.active, d.passive = d.passive, d.active }

// DBStats is a collection of statistics about a database.
type DBStats struct {
	NumSet uint64      // number of set records.
	LenSet memory.Size // sum of lengths in set records.
	AvgSet float64     // average size of length of records.

	NumTrash uint64      // number of set trash records.
	LenTrash memory.Size // sum of lengths in set trash records.
	AvgTrash float64     // average size of length of trash records.

	NumTTL uint64      // number of set records with expiration but not trash.
	LenTTL memory.Size // sum of lengths in set records with expiration but not trash.
	AvgTTL float64     // average size of length of records with expiration but not trash.

	NumSlots  uint64      // total number of records available.
	TableSize memory.Size // total number of bytes in the hash table.
	Load      float64     // percent of slots that are set.

	NumLogs    uint64      // total number of log files.
	LenLogs    memory.Size // total number of bytes in the log files.
	NumLogsTTL uint64      // total number of log files with ttl set.
	LenLogsTTL memory.Size // total number of bytes in log files with ttl set.

	SetPercent   float64 // percent of bytes that are set in the log files.
	TrashPercent float64 // percent of bytes that are trash in the log files.
	TTLPercent   float64 // percent of bytes that have expiration but not trash in the log files.

	Compacting      bool        // if true, a background compaction is in progress.
	Compactions     uint64      // total number of compactions that finished on either store.
	Active          int         // which store is currently active
	LogsRewritten   uint64      // total number of log files attempted to be rewritten.
	DataRewritten   memory.Size // total number of bytes of data rewritten.
	DataReclaimed   memory.Size // number of bytes reclaimed in the log files.
	DataReclaimable memory.Size // number of bytes potentially reclaimable in the log files.
}

// Stats returns statistics about the database and underlying stores.
func (d *DB) Stats() (DBStats, StoreStats, StoreStats) {
	d.mu.Lock()
	s0, s1, active := d.active, d.passive, 0
	compacting := d.compact != nil
	d.mu.Unlock()

	// sort them so s0 and s1 always get the same tag values.
	if s1.logsPath < s0.logsPath {
		s0, s1, active = s1, s0, 1
	}

	s0st := s0.Stats()
	s1st := s1.Stats()

	return DBStats{
		NumSet: s0st.Table.NumSet + s1st.Table.NumSet,
		LenSet: s0st.Table.LenSet + s1st.Table.LenSet,
		AvgSet: safeDivide(float64(s0st.Table.LenSet+s1st.Table.LenSet), float64(s0st.Table.NumSet+s1st.Table.NumSet)),

		NumTrash: s0st.Table.NumTrash + s1st.Table.NumTrash,
		LenTrash: s0st.Table.LenTrash + s1st.Table.LenTrash,
		AvgTrash: safeDivide(float64(s0st.Table.LenTrash+s1st.Table.LenTrash), float64(s0st.Table.NumTrash+s1st.Table.NumTrash)),

		NumTTL: s0st.Table.NumTTL + s1st.Table.NumTTL,
		LenTTL: s0st.Table.LenTTL + s1st.Table.LenTTL,
		AvgTTL: safeDivide(float64(s0st.Table.LenTTL+s1st.Table.LenTTL), float64(s0st.Table.NumTTL+s1st.Table.NumTTL)),

		NumSlots:  s0st.Table.NumSlots + s1st.Table.NumSlots,
		TableSize: s0st.Table.TableSize + s1st.Table.TableSize,
		Load:      safeDivide(float64(s0st.Table.NumSet+s1st.Table.NumSet), float64(s0st.Table.NumSlots+s1st.Table.NumSlots)),

		NumLogs:    s0st.NumLogs + s1st.NumLogs,
		LenLogs:    s0st.LenLogs + s1st.LenLogs,
		NumLogsTTL: s0st.NumLogsTTL + s1st.NumLogsTTL,
		LenLogsTTL: s0st.LenLogsTTL + s1st.LenLogsTTL,

		SetPercent:   safeDivide(float64(s0st.Table.LenSet+s1st.Table.LenSet), float64(s0st.LenLogs+s1st.LenLogs)),
		TrashPercent: safeDivide(float64(s0st.Table.LenTrash+s1st.Table.LenTrash), float64(s0st.LenLogs+s1st.LenLogs)),
		TTLPercent:   safeDivide(float64(s0st.Table.LenTTL+s1st.Table.LenTTL), float64(s0st.LenLogs+s1st.LenLogs)),

		Compacting:      compacting,
		Compactions:     s0st.Compactions + s1st.Compactions,
		Active:          active,
		LogsRewritten:   s0st.LogsRewritten + s1st.LogsRewritten,
		DataRewritten:   s0st.DataRewritten + s1st.DataRewritten,
		DataReclaimed:   s0st.DataReclaimed + s1st.DataReclaimed,
		DataReclaimable: s0st.DataReclaimable + s1st.DataReclaimable,
	}, s0st, s1st
}

// Close closes down the database and blocks until all background processes have stopped.
func (d *DB) Close() error {
	d.cloMu.Lock()
	defer d.cloMu.Unlock()

	if !d.closed.Set(Error.New("db closed")) {
		return d.cloErr
	}

	d.mu.Lock()
	compact := d.compact
	d.mu.Unlock()

	// if we have an active compaction, cancel and wait for it.
	if compact != nil {
		compact.cancel()
		compact.done.Wait()
	}

	// close down the stores now that compaction is finished.
	var eg errs.Group
	if d.active != nil {
		eg.Add(d.active.Close())
	}
	if d.passive != nil {
		eg.Add(d.passive.Close())
	}

	// wait for any background goroutines to finish.
	d.wg.Wait()

	d.cloErr = eg.Err()
	return d.cloErr
}

// Create adds an entry to the database with the given key and expiration time. Close or Cancel
// must be called on the Writer when done. It is safe to call either of them multiple times.
func (d *DB) Create(ctx context.Context, key Key, expires time.Time) (_ *Writer, err error) {
	defer mon.Task()(&ctx)(&err)

	d.mu.Lock()
	defer d.mu.Unlock()

	if err := signalError(&d.closed); err != nil {
		return nil, err
	}

	for {
		// if our load is lower than the compact load, we're good to create.
		load := d.active.Load()
		if load < db_CompactLoad {
			break
		}

		// check if we have a compaction going.
		if state := d.compact; state != nil {
			// if the load is low enough, we can still insert into the active store.
			if load < db_MaxLoad {
				break
			}

			// otherwise, the insertion may fail, so wait for it to finish. we drop the lock so that
			// Close can proceed, but this means that by the time we relock, we have to recheck the
			// load factor which is why this is a for loop.
			d.mu.Unlock()
			err := d.waitOnState(ctx, state)
			d.mu.Lock()

			if err != nil {
				return nil, err
			}
			continue
		}

		// no compaction in progress already when one is indicated by the load, so swap active and
		// begin the compaction.
		d.swapStoresLocked()
		d.beginPassiveCompaction()
	}

	return d.active.Create(ctx, key, expires)
}

func (d *DB) waitOnState(ctx context.Context, state *compactState) (err error) {
	defer mon.Task()(&ctx)(&err)

	// check if we're already closed so we don't have to worry about select nondeterminism: a closed
	// db or already canceled context will definitely error.
	if err := signalError(&d.closed); err != nil {
		return err
	} else if err := ctx.Err(); err != nil {
		return err
	}
	select {
	case <-ctx.Done():
		return ctx.Err()
	case <-d.closed.Signal():
		return signalError(&d.closed)
	case <-state.done.Signal():
		return nil
	}
}

// Read returns a reader for the given key. If the key is not present the returned Reader will be
// nil and the error will be a wrapped fs.ErrNotExist. Close must be called on the non-nil Reader
// when done.
func (d *DB) Read(ctx context.Context, key Key) (r *Reader, err error) {
	defer mon.Task()(&ctx)(&err)

	if err := signalError(&d.closed); err != nil {
		return nil, err
	}

	d.mu.Lock()
	first, second := d.active, d.passive
	d.mu.Unlock()

	rFirst, err := first.Read(ctx, key)
	if err != nil {
		return nil, err
	} else if rFirst != nil && !rFirst.Trash() {
		return rFirst, nil
	}
	defer func() {
		if r != rFirst && rFirst != nil {
			rFirst.Release()
		}
	}()

	rSecond, err := second.Read(ctx, key)
	if err != nil {
		return nil, err
	} else if rSecond != nil && !rSecond.Trash() {
		return rSecond, nil
	}
	defer func() {
		if r != rSecond && rSecond != nil {
			rSecond.Release()
		}
	}()

	// we either have no readers or at least one of them is trash. try to revive them before
	// returning.

	revive := func(r *Reader) {
		if r == nil || !r.Trash() {
			return
		}
		log := r.s.log.With(zap.String("record", r.rec.String()))
		log.Warn("trashed record was read")
		if err := r.Revive(ctx); err != nil {
			log.Error("unable to revive record", zap.Error(err))
		}
	}

	revive(rFirst)
	revive(rSecond)

	if rFirst != nil {
		return rFirst, nil
	}

	if rSecond != nil {
		return rSecond, nil
	}

	return nil, Error.Wrap(fs.ErrNotExist)
}

// Compact observes the result of compaction of both stores and returns the combined errors. If a
// compaction is ongoing when it is called, it uses the result of that compaction.
func (d *DB) Compact(ctx context.Context) (err error) {
	defer mon.Task()(&ctx)(&err)

	if err := signalError(&d.closed); err != nil {
		return err
	}

	var eg errs.Group
	compacted := make(map[*Store]struct{})

	for len(compacted) != 2 {
		compact := func() *compactState {
			d.mu.Lock()
			defer d.mu.Unlock()

			if compact := d.compact; compact != nil {
				// if we have a compaction going, observe the result of that.
				return compact
			}

			// otherwise, we want to compact the passive store. if we already have a result for the
			// current passive store, swap it before beginning the compaction.
			if _, ok := compacted[d.passive]; ok {
				d.swapStoresLocked()
			}

			return d.beginPassiveCompaction()
		}()

		// we should always have a compaction ongoing at this point that we're waiting for. it's
		// possible that it's for a store we have already observed, but that's ok. we'll just
		// observe it again.
		if compact == nil {
			return Error.New("programmer error: could not begin passive compaction")
		}

		select {
		case <-ctx.Done():
			return ctx.Err()
		case <-d.closed.Signal():
			return signalError(&d.closed)
		case <-compact.done.Signal():
			eg.Add(compact.done.Err())
			compacted[compact.store] = struct{}{}
		}
	}

	return eg.Err()
}

func (d *DB) beginPassiveCompaction() *compactState {
	// sanity check: don't overwrite an existing compaction. this is a programmer error. we don't
	// panic or anything because the code kinda assumes that the stores are arbitrarily loaded in
	// many places, so skipping this compaction isn't the end of the world.
	if d.compact != nil {
		return nil
	}

	ctx, cancel := context.WithCancel(context.Background())
	d.compact = &compactState{
		store:  d.passive,
		cancel: cancel,
	}
	go d.performPassiveCompaction(ctx, d.compact)
	return d.compact
}

// backgroundCompactions polls periodically while the db is not closed to compact stores if they
// need it.
func (d *DB) backgroundCompactions() {
	defer d.wg.Done()

	const (
		avgMinutes = 12 * 60 // 12 hours so that s0 and s1 are compacted once per day.
		maxMinutes = 2 * avgMinutes
	)

	rng := mwc.Rand()
	mins := 0

	for {
		select {
		case <-d.closed.Signal():
			return

		case <-time.After(time.Minute):
			// perform a background compaction approximately once per avgMinutes but at least once
			// per maxMinutes.

			mins++
			if rng.Intn(avgMinutes) == 0 || mins >= maxMinutes {
				mins = 0
				d.checkBackgroundCompactions()
			}
		}
	}
}

func (d *DB) checkBackgroundCompactions() {
	d.mu.Lock()
	defer d.mu.Unlock()

	// if there's already a compaction going, don't start another one.
	if d.compact != nil {
		return
	}

	storeAge := func(s *Store) uint32 {
		stats := s.Stats()
		if stats.Compacting {
			return 0
		} else if stats.LastCompact > 0 {
			return stats.Today - stats.LastCompact
		} else {
			return stats.Today - stats.Table.Created
		}
	}

	// we will compact whichever store has the oldest compaction as tracked by both the table
	// creation and last compact timestamp. it will only compact if the age is greater than 0
	// days.
	activeAge, passiveAge := storeAge(d.active), storeAge(d.passive)

	// if both stores are compacted on the same day, we don't compact either.
	if activeAge == 0 && passiveAge == 0 {
		return
	}

	// if the active store is older, make it passive so that when we compact the passive store we
	// are compacting the older one.
	if activeAge > passiveAge {
		d.swapStoresLocked()
	}

	d.beginPassiveCompaction()
}

func (d *DB) performPassiveCompaction(ctx context.Context, compact *compactState) {
	var err error
	defer mon.Task()(&ctx)(&err)

	err = compact.store.Compact(ctx, d.shouldTrash, d.lastRestore(ctx))
	if err != nil {
		d.log.Error("compaction failed", zap.Error(err))
	}

	compact.cancel()
	compact.done.Set(err)

	d.mu.Lock()
	d.compact = nil
	d.mu.Unlock()
}
